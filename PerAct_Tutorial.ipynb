{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNovH9jQPdTJ"
      },
      "source": [
        "# **PerAct**\n",
        "### Annotated Tutorial\n",
        "\n",
        "**Important: Before starting, change the runtime to GPU.**\n",
        "\n",
        "This notebook is an annotated tutorial on training [Perceiver-Actor (PerAct)](https://peract.github.io/) from scratch. We will look at training a single-task agent on the `open_drawer` task.  The tutorial will start from loading calibrated RGB-D images, and end with visualizing *action detections* in voxelized observations. Overall, this guide is\n",
        "meant to complement the [paper](https://peract.github.io/) by providing concrete implementation details.  \n",
        "\n",
        "### Full Code\n",
        "See [this Github repository](https://github.com/peract/peract) for the full code, pre-trained checkpoints, and pre-generated datasets. You should be able to use the pre-generated datasets with this notebook.\n",
        "\n",
        "### Credit\n",
        "This notebook heavily builds on data-loading and pre-preprocessing code from [`ARM`](https://github.com/stepjam/ARM), [`YARR`](https://github.com/stepjam/YARR), [`PyRep`](https://github.com/stepjam/PyRep), [`RLBench`](https://github.com/stepjam/RLBench) by [James et al.](https://stepjam.github.io/) The [PerceiverIO](https://arxiv.org/abs/2107.14795) code is adapted from [`perceiver-pytorch`](https://github.com/lucidrains/perceiver-pytorch) by [Phil Wang (lucidrains)](https://github.com/lucidrains). The optimizer is based on [this LAMB implementation](https://github.com/cybertronai/pytorch-lamb). See the corresponding licenses below.\n",
        "\n",
        "<img src=\"https://peract.github.io/media/figures/sim_task.jpg\" alt=\"drawing\" style=\"width:100px;\"/>\n",
        "\n",
        "### Licenses\n",
        "- [PerAct License (Apache 2.0)](https://github.com/peract/peract/blob/main/LICENSE) - Perceiver-Actor Transformer\n",
        "- [ARM License](https://github.com/peract/peract/blob/main/ARM_LICENSE) - Voxelization and Data Preprocessing\n",
        "- [YARR Licence (Apache 2.0)](https://github.com/stepjam/YARR/blob/main/LICENSE)\n",
        "- [RLBench Licence](https://github.com/stepjam/RLBench/blob/master/LICENSE)\n",
        "- [PyRep License (MIT)](https://github.com/stepjam/PyRep/blob/master/LICENSE)\n",
        "- [Perceiver PyTorch License (MIT)](https://github.com/lucidrains/perceiver-pytorch/blob/main/LICENSE)\n",
        "- [LAMB License (MIT)](https://github.com/cybertronai/pytorch-lamb/blob/master/LICENSE)\n",
        "- [CLIP License (MIT)](https://github.com/openai/CLIP/blob/main/LICENSE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY8iX-eLYepQ"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jM6GveIUIlW",
        "outputId": "92e2b6d9-9d42-4dc5-dd44-33b19d9a52d1"
      },
      "outputs": [],
      "source": [
        "# !pip install scipy ftfy regex tqdm torch git+https://github.com/openai/CLIP.git einops pyrender==0.1.45 trimesh==3.9.34 pycollada==0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGIz2RohYkJ9"
      },
      "source": [
        "### Clone Repo and Setup\n",
        "\n",
        "Clone [https://github.com/peract/peract_colab.git](github.com/peract/peract_colab.git).   \n",
        "\n",
        "This repo contains barebones code from [`ARM`](https://github.com/stepjam/ARM), [`YARR`](https://github.com/stepjam/YARR), [`PyRep`](https://github.com/stepjam/PyRep), [`RLBench`](https://github.com/stepjam/RLBench) to get started with  PerAct without the actual [V-REP](https://www.coppeliarobotics.com/) simulator.\n",
        "\n",
        "The repo also contains a pre-generated RLBench dataset of 10 expert demonstrations for the `open_drawer` task. This task has three variations: \"open the top drawer\", \"open the middle drawer\", and \"open the bottom drawer\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJU62Vd5kJ99",
        "outputId": "4921517a-0bf7-443a-923b-0e50c0fa2525"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/peract/peract_colab.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avT7JfBKc3Ya"
      },
      "source": [
        "If you fork-off this repo, you might want to pull the latest changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cERy2oElEuT",
        "outputId": "5580cb61-63f6-447c-fd9b-a5a394a60c9e"
      },
      "outputs": [],
      "source": [
        "# !cd peract_colab && git pull origin master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wchWWR0k0ufw"
      },
      "source": [
        "Set `PYOPENGL_PLATFORM=egl` for pyrender visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hCQorAMoc2Jh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "os.environ[\"DISPLAY\"] = \":0\"\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmZg4Qgt1Bf3"
      },
      "source": [
        "Define some constants and setting variables.\n",
        "\n",
        "The `BATCH_SIZE` is 1 to fit the model on a single GPU. But you can play around with the voxel sizes and Transformer layers to increase this.  \n",
        "\n",
        "In the paper, we use `NUM_LATENTS=2048` by default, but smaller latents like `512` are also fine (see Appendix G)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvkDxdGtgtk-"
      },
      "outputs": [],
      "source": [
        "# constants\n",
        "TASK = 'open_drawer'\n",
        "DATA_FOLDER ='peract_colab/data'\n",
        "EPISODES_FOLDER = 'colab_dataset/open_drawer/all_variations/episodes'\n",
        "EPISODE_FOLDER = 'episode%d'\n",
        "CAMERAS = ['front', 'left_shoulder', 'right_shoulder', 'wrist']\n",
        "LOW_DIM_SIZE = 4   # {left_finger_joint, right_finger_joint, gripper_open, timestep}\n",
        "IMAGE_SIZE =  128  # 128x128 - if you want to use higher voxel resolutions like 200^3, you might want to regenerate the dataset with larger images\n",
        "VARIATION_DESCRIPTIONS_PKL = 'variation_descriptions.pkl' # the pkl file that contains language goals for each demonstration\n",
        "EPISODE_LENGTH = 10 # max steps for agents\n",
        "DEMO_AUGMENTATION_EVERY_N = 10 # sample n-th frame in demo\n",
        "ROTATION_RESOLUTION = 5 # degree increments per axis\n",
        "\n",
        "# settings\n",
        "VOXEL_SIZES = [100] # 100x100x100 voxels\n",
        "NUM_LATENTS = 512 # PerceiverIO latents\n",
        "SCENE_BOUNDS = [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6] # [x_min, y_min, z_min, x_max, y_max, z_max] - the metric volume to be voxelized\n",
        "BATCH_SIZE = 1\n",
        "NUM_DEMOS = 8 # total number of training demonstrations to use while training PerAct\n",
        "NUM_TEST = 2 # episodes to evaluate on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttvy-2W12Dk6"
      },
      "source": [
        "Add `peract_colab` to the system path and make a directory for storing the replay buffer.  For now, we will store the replay buffer on disk to avoid memory issues with putting everthing on RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZt5BxKgngP6"
      },
      "outputs": [],
      "source": [
        "sys.path.append('peract_colab')\n",
        "data_path = os.path.join(DATA_FOLDER, EPISODES_FOLDER)\n",
        "\n",
        "train_replay_storage_dir = 'replay_train'\n",
        "if not os.path.exists(train_replay_storage_dir):\n",
        "  os.mkdir(train_replay_storage_dir)\n",
        "\n",
        "test_replay_storage_dir = 'replay_test'\n",
        "if not os.path.exists(test_replay_storage_dir):\n",
        "  os.mkdir(test_replay_storage_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG3WRcmfBKRB"
      },
      "source": [
        "## Data Loading & Preprocessing\n",
        "\n",
        "An expert demonstration recorded at ~20Hz contains 100s of individual timesteps in a sequence. Each timestep contains observations recorded from 4 calibrated cameras (front, left_shoulder, right_shoulder, and wrist) and other proprioception sensors. \"Calibrated\" means we know the extrinsics and intrinsics.\n",
        "\n",
        "Let's take a look at what these observations look like. Play around with different `episode_idx_to_visualize` and timesteps `ts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "DcqP4UMLVjyQ",
        "outputId": "61e8a4f4-77c2-4dd3-90a4-c21b792eda5e"
      },
      "outputs": [],
      "source": [
        "from rlbench.utils import get_stored_demo\n",
        "from rlbench.backend.utils import extract_obs\n",
        "\n",
        "# what to visualize\n",
        "episode_idx_to_visualize = 1 # out of 10 demos\n",
        "ts = 50 # timestep out of total timesteps\n",
        "\n",
        "# get demo\n",
        "demo = get_stored_demo(data_path=data_path,\n",
        "                      index=episode_idx_to_visualize)\n",
        "\n",
        "# extract obs at timestep\n",
        "obs_dict = extract_obs(demo._observations[ts], CAMERAS, t=ts)\n",
        "\n",
        "# total timesteps in demo\n",
        "print(f\"Demo {episode_idx_to_visualize} | {len(demo._observations)} total steps\\n\")\n",
        "\n",
        "# plot rgb and depth at timestep\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "rows, cols = 2, len(CAMERAS)\n",
        "\n",
        "plot_idx = 1\n",
        "for camera in CAMERAS:\n",
        "  # rgb\n",
        "  rgb_name = \"%s_%s\" % (camera, 'rgb')\n",
        "  rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
        "  fig.add_subplot(rows, cols, plot_idx)\n",
        "  plt.imshow(rgb)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"%s_rgb | step %s\" % (camera, ts))\n",
        "\n",
        "  # depth\n",
        "  depth_name = \"%s_%s\" % (camera, 'depth')\n",
        "  depth = np.transpose(obs_dict[depth_name], (1, 2, 0)).reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
        "  fig.add_subplot(rows, cols, plot_idx+len(CAMERAS))\n",
        "  plt.imshow(depth)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"%s_depth | step %s\" % (camera, ts))\n",
        "\n",
        "  plot_idx += 1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEqEV0mT3Tda"
      },
      "source": [
        "### Create Replay Buffer\n",
        "\n",
        "As described in **Section 3.4** of the paper, PerAct is trained with discrete-time input-action tuples from a dataset of demonstrations. These tuples are stored in a Replay Buffer following the [`ARM`](https://github.com/stepjam/ARM) codebase. You can use your own storage format, but here we follow `ARM` to benchmark against baselines and other methods by James et al.\n",
        "\n",
        "This replay buffer stores **<observation, language goal, keyframe action>** tuples sampled from demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghllJQShYW-S",
        "outputId": "7e3dafd7-e68b-4ca9-f9c1-6236e31371cd"
      },
      "outputs": [],
      "source": [
        "# Adapted from: https://github.com/stepjam/ARM/blob/main/arm/c2farm/launch_utils.py\n",
        "\n",
        "from yarr.utils.observation_type import ObservationElement\n",
        "from yarr.replay_buffer import ReplayElement, ReplayBuffer\n",
        "from yarr.replay_buffer.uniform_replay_buffer import UniformReplayBuffer\n",
        "\n",
        "\n",
        "def create_replay(batch_size: int,\n",
        "                  timesteps: int,\n",
        "                  save_dir: str,\n",
        "                  cameras: list,\n",
        "                  voxel_sizes,\n",
        "                  replay_size=3e5):\n",
        "\n",
        "    trans_indicies_size = 3 * len(voxel_sizes)\n",
        "    rot_and_grip_indicies_size = (3 + 1)\n",
        "    gripper_pose_size = 7\n",
        "    ignore_collisions_size = 1\n",
        "    max_token_seq_len = 77\n",
        "    lang_feat_dim = 1024\n",
        "    lang_emb_dim = 512\n",
        "\n",
        "    # low_dim_state\n",
        "    observation_elements = []\n",
        "    observation_elements.append(\n",
        "        ObservationElement('low_dim_state', (LOW_DIM_SIZE,), np.float32))\n",
        "\n",
        "    # rgb, depth, point cloud, intrinsics, extrinsics\n",
        "    for cname in cameras:\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_rgb' % cname, (3, IMAGE_SIZE, IMAGE_SIZE,), np.float32))\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_depth' % cname, (1, IMAGE_SIZE, IMAGE_SIZE,), np.float32))\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_point_cloud' % cname, (3, IMAGE_SIZE, IMAGE_SIZE,), np.float32)) # see pyrep/objects/vision_sensor.py on how pointclouds are extracted from depth frames\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_camera_extrinsics' % cname, (4, 4,), np.float32))\n",
        "        observation_elements.append(\n",
        "            ObservationElement('%s_camera_intrinsics' % cname, (3, 3,), np.float32))\n",
        "\n",
        "    # discretized translation, discretized rotation, discrete ignore collision, 6-DoF gripper pose, and pre-trained language embeddings\n",
        "    observation_elements.extend([\n",
        "        ReplayElement('trans_action_indicies', (trans_indicies_size,),\n",
        "                      np.int32),\n",
        "        ReplayElement('rot_grip_action_indicies', (rot_and_grip_indicies_size,),\n",
        "                      np.int32),\n",
        "        ReplayElement('ignore_collisions', (ignore_collisions_size,),\n",
        "                      np.int32),\n",
        "        ReplayElement('gripper_pose', (gripper_pose_size,),\n",
        "                      np.float32),\n",
        "        ReplayElement('lang_goal_embs', (max_token_seq_len, lang_emb_dim,), # extracted from CLIP's language encoder\n",
        "                      np.float32),\n",
        "        ReplayElement('lang_goal', (1,), object), # language goal string for debugging and visualization\n",
        "    ])\n",
        "\n",
        "    extra_replay_elements = [\n",
        "        ReplayElement('demo', (), np.bool),\n",
        "    ]\n",
        "\n",
        "    replay_buffer = UniformReplayBuffer( # all tuples in the buffer have equal sample weighting\n",
        "        save_dir=save_dir,\n",
        "        batch_size=batch_size,\n",
        "        timesteps=timesteps,\n",
        "        replay_capacity=int(replay_size),\n",
        "        action_shape=(8,), # 3 translation + 4 rotation quaternion + 1 gripper open\n",
        "        action_dtype=np.float32,\n",
        "        reward_shape=(),\n",
        "        reward_dtype=np.float32,\n",
        "        update_horizon=1,\n",
        "        observation_elements=observation_elements,\n",
        "        extra_replay_elements=extra_replay_elements\n",
        "    )\n",
        "    return replay_buffer\n",
        "\n",
        "train_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
        "                                    timesteps=1,\n",
        "                                    save_dir=train_replay_storage_dir,\n",
        "                                    cameras=CAMERAS,\n",
        "                                    voxel_sizes=VOXEL_SIZES)\n",
        "\n",
        "test_replay_buffer = create_replay(batch_size=BATCH_SIZE,\n",
        "                                   timesteps=1,\n",
        "                                   save_dir=test_replay_storage_dir,\n",
        "                                   cameras=CAMERAS,\n",
        "                                   voxel_sizes=VOXEL_SIZES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GeqBQG03vTG"
      },
      "source": [
        "### Fill Replay with Demos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Spr_Tk0Z0tP"
      },
      "source": [
        "#### Keyframe Extraction\n",
        "\n",
        "Instead of directly trying to predict every action in the demonstration, which could be very noisy and inefficient, we extract keyframe actions that capture **bottleneck** poses \\[[James et al.](https://arxiv.org/abs/2105.14829)\\]. This extraction is done with a simple heuristic: an action is a keyframe action if (1) the joint-velocities are near zero and (2) the gripper open state has not changed. Then every timestep in the demonstration can be cast as a predict \"the next (best) keyframe\" classification task, like the orange points in this figure:  \n",
        "\n",
        "<div>\n",
        "<img src=\"https://peract.github.io/media/figures/keypoints.jpg\" alt=\"drawing\"  width=\"300\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsyOYB_v6VNk"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/stepjam/ARM/blob/main/arm/demo_loading_utils.py\n",
        "\n",
        "from rlbench.demo import Demo\n",
        "from typing import List\n",
        "\n",
        "def _is_stopped(demo, i, obs, stopped_buffer, delta=0.1):\n",
        "    next_is_not_final = i == (len(demo) - 2)\n",
        "    gripper_state_no_change = (\n",
        "            i < (len(demo) - 2) and\n",
        "            (obs.gripper_open == demo[i + 1].gripper_open and\n",
        "             obs.gripper_open == demo[i - 1].gripper_open and\n",
        "             demo[i - 2].gripper_open == demo[i - 1].gripper_open))\n",
        "    small_delta = np.allclose(obs.joint_velocities, 0, atol=delta)\n",
        "    stopped = (stopped_buffer <= 0 and small_delta and\n",
        "               (not next_is_not_final) and gripper_state_no_change)\n",
        "    return stopped\n",
        "\n",
        "def _keypoint_discovery(demo: Demo,\n",
        "                        stopping_delta=0.1) -> List[int]:\n",
        "    episode_keypoints = []\n",
        "    prev_gripper_open = demo[0].gripper_open\n",
        "    stopped_buffer = 0\n",
        "    for i, obs in enumerate(demo):\n",
        "        stopped = _is_stopped(demo, i, obs, stopped_buffer, stopping_delta)\n",
        "        stopped_buffer = 4 if stopped else stopped_buffer - 1\n",
        "        # if change in gripper, or end of episode.\n",
        "        last = i == (len(demo) - 1)\n",
        "        if i != 0 and (obs.gripper_open != prev_gripper_open or\n",
        "                        last or stopped):\n",
        "            episode_keypoints.append(i)\n",
        "        prev_gripper_open = obs.gripper_open\n",
        "    if len(episode_keypoints) > 1 and (episode_keypoints[-1] - 1) == \\\n",
        "            episode_keypoints[-2]:\n",
        "        episode_keypoints.pop(-2)\n",
        "    print('Found %d keypoints.' % len(episode_keypoints), episode_keypoints)\n",
        "    return episode_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdipWAdGn3Pd"
      },
      "source": [
        "Let's take a look at what these keyframe actions look like.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "NHOsZUlPk-0z",
        "outputId": "c67fe360-bf50-476c-8c65-44714f4370cd"
      },
      "outputs": [],
      "source": [
        "# use Demo 1 like last time\n",
        "episode_idx_to_visualize = 1\n",
        "demo = get_stored_demo(data_path=data_path,\n",
        "                      index=episode_idx_to_visualize)\n",
        "\n",
        "# total timesteps\n",
        "print(\"Demo %s | %s total steps\" % (episode_idx_to_visualize, len(demo._observations)))\n",
        "\n",
        "# use the heuristic to extract keyframes (aka keypoints)\n",
        "episode_keypoints = _keypoint_discovery(demo)\n",
        "\n",
        "# visualize rgb observations from these keyframes\n",
        "for kp_idx, kp in enumerate(episode_keypoints):\n",
        "    obs_dict = extract_obs(demo._observations[kp], CAMERAS, t=kp)\n",
        "\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    rgb_name = \"front_rgb\"\n",
        "    rgb = np.transpose(obs_dict[rgb_name], (1, 2, 0))\n",
        "    plt.imshow(rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"front_rgb | step %s | keypoint %s \" % (kp, kp_idx))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlvQXw-7oQc5"
      },
      "source": [
        "Notice that the motion-planner used to generate demonstrations might take various paths to execute the \"opening\" motion, but all paths strictly pass through these **bottleneck** poses, since that's how the expert demonstrations were collected in RLBench. This essentially circuments the issue of training directly on randomized motion paths from sampling-based motion planners, which can be quite noisy to learn from for end-to-end methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3NW3dKQpx98"
      },
      "source": [
        "#### Fill Replay\n",
        "\n",
        "Some helper functions for filling the replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db_0S-7UpN1d"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "import arm.utils as utils\n",
        "\n",
        "from rlbench.backend.observation import Observation\n",
        "from yarr.replay_buffer.replay_buffer import ReplayBuffer\n",
        "\n",
        "# discretize translation, rotation, gripper open, and ignore collision actions\n",
        "def _get_action(\n",
        "        obs_tp1: Observation,\n",
        "        obs_tm1: Observation,\n",
        "        rlbench_scene_bounds: List[float], # metric 3D bounds of the scene\n",
        "        voxel_sizes: List[int],\n",
        "        rotation_resolution: int,\n",
        "        crop_augmentation: bool):\n",
        "    quat = utils.normalize_quaternion(obs_tp1.gripper_pose[3:])\n",
        "    if quat[-1] < 0:\n",
        "        quat = -quat\n",
        "    disc_rot = utils.quaternion_to_discrete_euler(quat, rotation_resolution)\n",
        "    attention_coordinate = obs_tp1.gripper_pose[:3]\n",
        "    trans_indicies, attention_coordinates = [], []\n",
        "    bounds = np.array(rlbench_scene_bounds)\n",
        "    ignore_collisions = int(obs_tm1.ignore_collisions)\n",
        "    for depth, vox_size in enumerate(voxel_sizes): # only single voxelization-level is used in PerAct\n",
        "        index = utils.point_to_voxel_index(\n",
        "            obs_tp1.gripper_pose[:3], vox_size, bounds)\n",
        "        trans_indicies.extend(index.tolist())\n",
        "        res = (bounds[3:] - bounds[:3]) / vox_size\n",
        "        attention_coordinate = bounds[:3] + res * index\n",
        "        attention_coordinates.append(attention_coordinate)\n",
        "\n",
        "    rot_and_grip_indicies = disc_rot.tolist()\n",
        "    grip = float(obs_tp1.gripper_open)\n",
        "    rot_and_grip_indicies.extend([int(obs_tp1.gripper_open)])\n",
        "    return trans_indicies, rot_and_grip_indicies, ignore_collisions, np.concatenate(\n",
        "        [obs_tp1.gripper_pose, np.array([grip])]), attention_coordinates\n",
        "\n",
        "# extract CLIP language features for goal string\n",
        "def _clip_encode_text(clip_model, text):\n",
        "    x = clip_model.token_embedding(text).type(clip_model.dtype)  # [batch_size, n_ctx, d_model]\n",
        "\n",
        "    x = x + clip_model.positional_embedding.type(clip_model.dtype)\n",
        "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "    x = clip_model.transformer(x)\n",
        "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "    x = clip_model.ln_final(x).type(clip_model.dtype)\n",
        "\n",
        "    emb = x.clone()\n",
        "    x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ clip_model.text_projection\n",
        "\n",
        "    return x, emb\n",
        "\n",
        "# add individual data points to replay\n",
        "def _add_keypoints_to_replay(\n",
        "        replay: ReplayBuffer,\n",
        "        inital_obs: Observation,\n",
        "        demo: Demo,\n",
        "        episode_keypoints: List[int],\n",
        "        cameras: List[str],\n",
        "        rlbench_scene_bounds: List[float],\n",
        "        voxel_sizes: List[int],\n",
        "        rotation_resolution: int,\n",
        "        crop_augmentation: bool,\n",
        "        description: str = '',\n",
        "        clip_model = None,\n",
        "        device = 'cpu'):\n",
        "    prev_action = None\n",
        "    obs = inital_obs\n",
        "    for k, keypoint in enumerate(episode_keypoints):\n",
        "        obs_tp1 = demo[keypoint]\n",
        "        obs_tm1 = demo[max(0, keypoint - 1)]\n",
        "        trans_indicies, rot_grip_indicies, ignore_collisions, action, attention_coordinates = _get_action(\n",
        "            obs_tp1, obs_tm1, rlbench_scene_bounds, voxel_sizes,\n",
        "            rotation_resolution, crop_augmentation)\n",
        "\n",
        "        terminal = (k == len(episode_keypoints) - 1)\n",
        "        reward = float(terminal) * 1.0 if terminal else 0\n",
        "\n",
        "        obs_dict = extract_obs(obs, CAMERAS, t=k, prev_action=prev_action)\n",
        "        tokens = clip.tokenize([description]).numpy()\n",
        "        token_tensor = torch.from_numpy(tokens).to(device)\n",
        "        lang_feats, lang_embs = _clip_encode_text(clip_model, token_tensor)\n",
        "        obs_dict['lang_goal_embs'] = lang_embs[0].float().detach().cpu().numpy()\n",
        "\n",
        "        prev_action = np.copy(action)\n",
        "\n",
        "        others = {'demo': True}\n",
        "        final_obs = {\n",
        "            'trans_action_indicies': trans_indicies,\n",
        "            'rot_grip_action_indicies': rot_grip_indicies,\n",
        "            'gripper_pose': obs_tp1.gripper_pose,\n",
        "            'lang_goal': np.array([description], dtype=object),\n",
        "        }\n",
        "\n",
        "        others.update(final_obs)\n",
        "        others.update(obs_dict)\n",
        "\n",
        "        timeout = False\n",
        "        replay.add(action, reward, terminal, timeout, **others)\n",
        "        obs = obs_tp1\n",
        "\n",
        "    # final step\n",
        "    obs_dict_tp1 = extract_obs(obs_tp1, CAMERAS, t=k + 1, prev_action=prev_action)\n",
        "    obs_dict_tp1['lang_goal_embs'] = lang_embs[0].float().detach().cpu().numpy()\n",
        "\n",
        "    obs_dict_tp1.pop('wrist_world_to_cam', None)\n",
        "    obs_dict_tp1.update(final_obs)\n",
        "    replay.add_final(**obs_dict_tp1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JoelGN-_a5O"
      },
      "source": [
        "Finally fill the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwFMLOG0pMEq"
      },
      "outputs": [],
      "source": [
        "def fill_replay(replay: ReplayBuffer,\n",
        "                start_idx: int,\n",
        "                num_demos: int,\n",
        "                demo_augmentation: bool,\n",
        "                demo_augmentation_every_n: int,\n",
        "                cameras: List[str],\n",
        "                rlbench_scene_bounds: List[float],  # AKA: DEPTH0_BOUNDS\n",
        "                voxel_sizes: List[int],\n",
        "                rotation_resolution: int,\n",
        "                crop_augmentation: bool,\n",
        "                clip_model = None,\n",
        "                device = 'cpu'):\n",
        "    print('Filling replay ...')\n",
        "    for d_idx in range(start_idx, start_idx+num_demos):\n",
        "        print(\"Filling demo %d\" % d_idx)\n",
        "        demo = get_stored_demo(data_path=data_path,\n",
        "                               index=d_idx)\n",
        "\n",
        "        # get language goal from disk\n",
        "        varation_descs_pkl_file = os.path.join(data_path, EPISODE_FOLDER % d_idx, VARIATION_DESCRIPTIONS_PKL)\n",
        "        with open(varation_descs_pkl_file, 'rb') as f:\n",
        "          descs = pickle.load(f)\n",
        "\n",
        "        # extract keypoints\n",
        "        episode_keypoints = _keypoint_discovery(demo)\n",
        "\n",
        "        for i in range(len(demo) - 1):\n",
        "            if not demo_augmentation and i > 0:\n",
        "                break\n",
        "            if i % demo_augmentation_every_n != 0: # choose only every n-th frame\n",
        "                continue\n",
        "\n",
        "            obs = demo[i]\n",
        "            desc = descs[0]\n",
        "            # if our starting point is past one of the keypoints, then remove it\n",
        "            while len(episode_keypoints) > 0 and i >= episode_keypoints[0]:\n",
        "                episode_keypoints = episode_keypoints[1:]\n",
        "            if len(episode_keypoints) == 0:\n",
        "                break\n",
        "            _add_keypoints_to_replay(\n",
        "                replay, obs, demo, episode_keypoints, cameras,\n",
        "                rlbench_scene_bounds, voxel_sizes,\n",
        "                rotation_resolution, crop_augmentation, description=desc,\n",
        "                clip_model=clip_model, device=device)\n",
        "    print('Replay filled with demos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lPNE8Pjsmr3"
      },
      "source": [
        "Load a [pre-trained CLIP model](https://arxiv.org/abs/2103.00020) to extract language features. You can probably swap this with other language models, but CLIP's language features were trained to be aligned with image features, which might give it a multi-modal edge over text-only models 🤷"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrO9aVEXsRps",
        "outputId": "6a817b12-1924-487f-bd79-3802861f543f"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"RN50\", device=device) # CLIP-ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmKmx2U48vSJ",
        "outputId": "c95ab331-ea09-4551-f6fc-fea51845256d"
      },
      "outputs": [],
      "source": [
        "from yarr.replay_buffer.wrappers.pytorch_replay_buffer import PyTorchReplayBuffer\n",
        "\n",
        "print(\"-- Train Buffer --\")\n",
        "fill_replay(replay=train_replay_buffer,\n",
        "            start_idx=0,\n",
        "            num_demos=NUM_DEMOS,\n",
        "            demo_augmentation=True,\n",
        "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
        "            cameras=CAMERAS,\n",
        "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
        "            voxel_sizes=VOXEL_SIZES,\n",
        "            rotation_resolution=ROTATION_RESOLUTION,\n",
        "            crop_augmentation=False,\n",
        "            clip_model=clip_model,\n",
        "            device=device)\n",
        "\n",
        "print(\"-- Test Buffer --\")\n",
        "fill_replay(replay=test_replay_buffer,\n",
        "            start_idx=NUM_DEMOS,\n",
        "            num_demos=NUM_TEST,\n",
        "            demo_augmentation=True,\n",
        "            demo_augmentation_every_n=DEMO_AUGMENTATION_EVERY_N,\n",
        "            cameras=CAMERAS,\n",
        "            rlbench_scene_bounds=SCENE_BOUNDS,\n",
        "            voxel_sizes=VOXEL_SIZES,\n",
        "            rotation_resolution=ROTATION_RESOLUTION,\n",
        "            crop_augmentation=False,\n",
        "            clip_model=clip_model,\n",
        "            device=device)\n",
        "\n",
        "# delete the CLIP model since we have already extracted language features\n",
        "del clip_model\n",
        "\n",
        "# wrap buffer with PyTorch dataset and make iterator\n",
        "train_wrapped_replay = PyTorchReplayBuffer(train_replay_buffer)\n",
        "train_dataset = train_wrapped_replay.dataset()\n",
        "train_data_iter = iter(train_dataset)\n",
        "\n",
        "test_wrapped_replay = PyTorchReplayBuffer(test_replay_buffer)\n",
        "test_dataset = test_wrapped_replay.dataset()\n",
        "test_data_iter = iter(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVbNjqUAxFv"
      },
      "source": [
        "## Training PerAct\n",
        "\n",
        "### Voxelization\n",
        "\n",
        "Now we define a class for voxelizing calibrated RGB-D observations following [C2FARM \\(James et al.\\)](https://arxiv.org/pdf/2106.12534.pdf)\n",
        "\n",
        "The input to the voxelizer is:\n",
        "- Flattened RGB images\n",
        "- Flattened global-coordinate point clouds\n",
        "- Scene bounds in metric units that specify the volume to be voxelized\n",
        "\n",
        "The output is a 10-dimensional voxel grid (see Appendix B for details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7UFToUQAr1t"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/stepjam/ARM/blob/main/arm/c2farm/voxel_grid.py\n",
        "\n",
        "from functools import reduce as funtool_reduce\n",
        "from operator import mul\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MIN_DENOMINATOR = 1e-12\n",
        "INCLUDE_PER_VOXEL_COORD = False\n",
        "\n",
        "\n",
        "class VoxelGrid(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 coord_bounds,\n",
        "                 voxel_size: int,\n",
        "                 device,\n",
        "                 batch_size,\n",
        "                 feature_size,\n",
        "                 max_num_coords: int,):\n",
        "        super(VoxelGrid, self).__init__()\n",
        "        self._device = device\n",
        "        self._voxel_size = voxel_size\n",
        "        self._voxel_shape = [voxel_size] * 3\n",
        "        self._voxel_d = float(self._voxel_shape[-1])\n",
        "        self._voxel_feature_size = 4 + feature_size\n",
        "        self._voxel_shape_spec = torch.tensor(self._voxel_shape,\n",
        "                                              device=device).unsqueeze(\n",
        "            0) + 2  # +2 because we crop the edges.\n",
        "        self._coord_bounds = torch.tensor(coord_bounds, dtype=torch.float,\n",
        "                                          device=device).unsqueeze(0)\n",
        "        max_dims = self._voxel_shape_spec[0]\n",
        "        self._total_dims_list = torch.cat(\n",
        "            [torch.tensor([batch_size], device=device), max_dims,\n",
        "             torch.tensor([4 + feature_size], device=device)], -1).tolist()\n",
        "        self._ones_max_coords = torch.ones((batch_size, max_num_coords, 1),\n",
        "                                           device=device)\n",
        "        self._num_coords = max_num_coords\n",
        "\n",
        "        shape = self._total_dims_list\n",
        "\n",
        "        self._result_dim_sizes = torch.tensor(\n",
        "            [funtool_reduce(mul, shape[i + 1:], 1) for i in range(len(shape) - 1)] + [\n",
        "                1], device=device)\n",
        "        flat_result_size = funtool_reduce(mul, shape, 1)\n",
        "\n",
        "        self._initial_val = torch.tensor(0, dtype=torch.float,\n",
        "                                         device=device)\n",
        "        self._flat_output = torch.ones(flat_result_size, dtype=torch.float,\n",
        "                                       device=device) * self._initial_val\n",
        "        self._arange_to_max_coords = torch.arange(4 + feature_size,\n",
        "                                                  device=device)\n",
        "        self._flat_zeros = torch.zeros(flat_result_size, dtype=torch.float,\n",
        "                                       device=device)\n",
        "\n",
        "        self._const_1 = torch.tensor(1.0, device=device)\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        # Coordinate Bounds:\n",
        "        self._bb_mins = self._coord_bounds[..., 0:3]\n",
        "        bb_maxs = self._coord_bounds[..., 3:6]\n",
        "        bb_ranges = bb_maxs - self._bb_mins\n",
        "        # get voxel dimensions. 'DIMS' mode\n",
        "        self._dims = dims = self._voxel_shape_spec.int()\n",
        "        self._dims_orig = dims_orig = self._voxel_shape_spec.int() - 2\n",
        "        self._dims_m_one = (dims - 1).int()\n",
        "        # BS x 1 x 3\n",
        "        self._res = bb_ranges / (dims_orig.float() + MIN_DENOMINATOR)\n",
        "        self._res_minis_2 = bb_ranges / (dims.float() - 2 + MIN_DENOMINATOR)\n",
        "\n",
        "        self._voxel_indicy_denmominator = self._res + MIN_DENOMINATOR\n",
        "        self._dims_m_one_zeros = torch.zeros_like(self._dims_m_one)\n",
        "\n",
        "        batch_indices = torch.arange(self._batch_size, dtype=torch.int,\n",
        "                                     device=device).view(self._batch_size, 1, 1)\n",
        "        self._tiled_batch_indices = batch_indices.repeat(\n",
        "            [1, self._num_coords, 1])\n",
        "\n",
        "        w = self._voxel_shape[0] + 2\n",
        "        arange = torch.arange(0, w, dtype=torch.float, device=device)\n",
        "        self._index_grid = torch.cat([\n",
        "            arange.view(w, 1, 1, 1).repeat([1, w, w, 1]),\n",
        "            arange.view(1, w, 1, 1).repeat([w, 1, w, 1]),\n",
        "            arange.view(1, 1, w, 1).repeat([w, w, 1, 1])], dim=-1).unsqueeze(\n",
        "            0).repeat([self._batch_size, 1, 1, 1, 1])\n",
        "\n",
        "    def _broadcast(self, src: torch.Tensor, other: torch.Tensor, dim: int):\n",
        "        if dim < 0:\n",
        "            dim = other.dim() + dim\n",
        "        if src.dim() == 1:\n",
        "            for _ in range(0, dim):\n",
        "                src = src.unsqueeze(0)\n",
        "        for _ in range(src.dim(), other.dim()):\n",
        "            src = src.unsqueeze(-1)\n",
        "        src = src.expand_as(other)\n",
        "        return src\n",
        "\n",
        "    def _scatter_mean(self, src: torch.Tensor, index: torch.Tensor, out: torch.Tensor,\n",
        "                      dim: int = -1):\n",
        "        out = out.scatter_add_(dim, index, src)\n",
        "\n",
        "        index_dim = dim\n",
        "        if index_dim < 0:\n",
        "            index_dim = index_dim + src.dim()\n",
        "        if index.dim() <= index_dim:\n",
        "            index_dim = index.dim() - 1\n",
        "\n",
        "        ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n",
        "        out_count = torch.zeros(out.size(), dtype=out.dtype, device=out.device)\n",
        "        out_count = out_count.scatter_add_(index_dim, index, ones)\n",
        "        out_count.clamp_(1)\n",
        "        count = self._broadcast(out_count, out, dim)\n",
        "        if torch.is_floating_point(out):\n",
        "            out.true_divide_(count)\n",
        "        else:\n",
        "            out.floor_divide_(count)\n",
        "        return out\n",
        "\n",
        "    def _scatter_nd(self, indices, updates):\n",
        "        indices_shape = indices.shape\n",
        "        num_index_dims = indices_shape[-1]\n",
        "        flat_updates = updates.view((-1,))\n",
        "        indices_scales = self._result_dim_sizes[0:num_index_dims].view(\n",
        "            [1] * (len(indices_shape) - 1) + [num_index_dims])\n",
        "        indices_for_flat_tiled = ((indices * indices_scales).sum(\n",
        "            dim=-1, keepdims=True)).view(-1, 1).repeat(\n",
        "            *[1, self._voxel_feature_size])\n",
        "\n",
        "        implicit_indices = self._arange_to_max_coords[\n",
        "                           :self._voxel_feature_size].unsqueeze(0).repeat(\n",
        "            *[indices_for_flat_tiled.shape[0], 1])\n",
        "        indices_for_flat = indices_for_flat_tiled + implicit_indices\n",
        "        flat_indices_for_flat = indices_for_flat.view((-1,)).long()\n",
        "\n",
        "        flat_scatter = self._scatter_mean(\n",
        "            flat_updates, flat_indices_for_flat,\n",
        "            out=torch.zeros_like(self._flat_output))\n",
        "        return flat_scatter.view(self._total_dims_list)\n",
        "\n",
        "    def coords_to_bounding_voxel_grid(self, coords, coord_features=None,\n",
        "                                      coord_bounds=None):\n",
        "        voxel_indicy_denmominator = self._voxel_indicy_denmominator\n",
        "        res, bb_mins = self._res, self._bb_mins\n",
        "        if coord_bounds is not None:\n",
        "            bb_mins = coord_bounds[..., 0:3]\n",
        "            bb_maxs = coord_bounds[..., 3:6]\n",
        "            bb_ranges = bb_maxs - bb_mins\n",
        "            res = bb_ranges / (self._dims_orig.float() + MIN_DENOMINATOR)\n",
        "            voxel_indicy_denmominator = res + MIN_DENOMINATOR\n",
        "\n",
        "        bb_mins_shifted = bb_mins - res  # shift back by one\n",
        "        floor = torch.floor(\n",
        "            (coords - bb_mins_shifted.unsqueeze(1)) / voxel_indicy_denmominator.unsqueeze(1)).int()\n",
        "        voxel_indices = torch.min(floor, self._dims_m_one)\n",
        "        voxel_indices = torch.max(voxel_indices, self._dims_m_one_zeros)\n",
        "\n",
        "        # global-coordinate point cloud (x, y, z)\n",
        "        voxel_values = coords\n",
        "\n",
        "        # rgb values (R, G, B)\n",
        "        if coord_features is not None:\n",
        "            voxel_values = torch.cat([voxel_values, coord_features], -1) # concat rgb values (B, 128, 128, 3)\n",
        "\n",
        "        # coordinates to aggregate over\n",
        "        _, num_coords, _ = voxel_indices.shape\n",
        "        all_indices = torch.cat([\n",
        "            self._tiled_batch_indices[:, :num_coords], voxel_indices], -1)\n",
        "\n",
        "        # max coordinates\n",
        "        voxel_values_pruned_flat = torch.cat(\n",
        "            [voxel_values, self._ones_max_coords[:, :num_coords]], -1)\n",
        "\n",
        "        # aggregate across camera views\n",
        "        scattered = self._scatter_nd(\n",
        "            all_indices.view([-1, 1 + 3]),\n",
        "            voxel_values_pruned_flat.view(-1, self._voxel_feature_size))\n",
        "\n",
        "        vox = scattered[:, 1:-1, 1:-1, 1:-1]\n",
        "        if INCLUDE_PER_VOXEL_COORD:\n",
        "            res_expanded = res.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "            res_centre = (res_expanded * self._index_grid) + res_expanded / 2.0\n",
        "            coord_positions = (res_centre + bb_mins_shifted.unsqueeze(\n",
        "                1).unsqueeze(1).unsqueeze(1))[:, 1:-1, 1:-1, 1:-1]\n",
        "            vox = torch.cat([vox[..., :-1], coord_positions, vox[..., -1:]], -1)\n",
        "\n",
        "        # occupied value\n",
        "        occupied = (vox[..., -1:] > 0).float()\n",
        "        vox = torch.cat([\n",
        "            vox[..., :-1], occupied], -1)\n",
        "\n",
        "        # hard voxel-location position encoding\n",
        "        return torch.cat(\n",
        "           [vox[..., :-1], self._index_grid[:, :-2, :-2, :-2] / self._voxel_d,\n",
        "            vox[..., -1:]], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nymM14ruQxiR"
      },
      "source": [
        "Let's try to use this voxelizer on observation samples from the replay buffer.\n",
        "\n",
        "But first, lets define some helper functions to normalize and format RGB and pointcloud input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv9Yplc8LBGt"
      },
      "outputs": [],
      "source": [
        "from arm.utils import stack_on_channel\n",
        "\n",
        "def _norm_rgb(x):\n",
        "    return (x.float() / 255.0) * 2.0 - 1.0\n",
        "\n",
        "def _preprocess_inputs(replay_sample):\n",
        "    obs, pcds = [], []\n",
        "    for n in CAMERAS:\n",
        "        rgb = stack_on_channel(replay_sample['%s_rgb' % n])\n",
        "        pcd = stack_on_channel(replay_sample['%s_point_cloud' % n])\n",
        "\n",
        "        rgb = _norm_rgb(rgb)\n",
        "\n",
        "        obs.append([rgb, pcd]) # obs contains both rgb and pointcloud (used in ARM for other baselines)\n",
        "        pcds.append(pcd) # only pointcloud\n",
        "    return obs, pcds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrP8sMevRaZv"
      },
      "source": [
        "The rgb and pointcloud inputs have to be flattened before feeding them into the voxelizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIlYMxU-Jgn5"
      },
      "outputs": [],
      "source": [
        "from arm.utils import visualise_voxel\n",
        "\n",
        "# initialize voxelizer\n",
        "vox_grid = VoxelGrid(\n",
        "    coord_bounds=SCENE_BOUNDS,\n",
        "    voxel_size=VOXEL_SIZES[0],\n",
        "    device=device,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    feature_size=3,\n",
        "    max_num_coords=np.prod([IMAGE_SIZE, IMAGE_SIZE]) * len(CAMERAS),\n",
        ")\n",
        "\n",
        "# sample from dataset\n",
        "batch = next(train_data_iter)\n",
        "lang_goal = batch['lang_goal'][0][0][0]\n",
        "batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
        "\n",
        "# preprocess observations\n",
        "obs, pcds = _preprocess_inputs(batch)\n",
        "\n",
        "# flatten observations\n",
        "bs = obs[0][0].shape[0]\n",
        "pcd_flat = torch.cat([p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in pcds], 1)\n",
        "\n",
        "image_features = [o[0] for o in obs]\n",
        "feat_size = image_features[0].shape[1]\n",
        "flat_imag_features = torch.cat(\n",
        "    [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in image_features], 1)\n",
        "\n",
        "# tensorize scene bounds\n",
        "bounds = torch.tensor(SCENE_BOUNDS, device=device).unsqueeze(0)\n",
        "\n",
        "# voxelize!\n",
        "voxel_grid = vox_grid.coords_to_bounding_voxel_grid(pcd_flat,\n",
        "                                                    coord_features=flat_imag_features,\n",
        "                                                    coord_bounds=bounds)\n",
        "\n",
        "# swap to channels fist\n",
        "vis_voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach().cpu().numpy()\n",
        "\n",
        "# expert action voxel indicies\n",
        "vis_gt_coord = batch['trans_action_indicies'][:, -1, :3].int().detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "SKo9PWhahZ5v",
        "outputId": "f014a287-af96-4c18-caf7-ed481ef5ba92"
      },
      "outputs": [],
      "source": [
        "# render voxel grid with expert action (blue)\n",
        "#@markdown #### Show voxel grid and expert action (blue)\n",
        "#@markdown Adjust `rotation_amount` to change the camera yaw angle for rendering.\n",
        "rotation_amount = 0 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
        "rendered_img = visualise_voxel(vis_voxel_grid[0],\n",
        "                               None,\n",
        "                               None,\n",
        "                               vis_gt_coord[0],\n",
        "                               voxel_size=0.045,\n",
        "                               rotation_amount=np.deg2rad(rotation_amount))\n",
        "\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "plt.imshow(rendered_img)\n",
        "plt.axis('off')\n",
        "\n",
        "print(f\"Lang goal: {lang_goal}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK7qhjkcSqhC"
      },
      "source": [
        "This visualization shows a voxel grid of size 100x100x100 = 1 million voxels, and one expert keyframe action (blue voxel). These samples are what PerAct is trained with. Given a language goal and voxel grid, we train a detector to detect the next best action with supervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeMSx_7pNouL"
      },
      "source": [
        "### PerceiverIO\n",
        "\n",
        "Now we can start implementing the actual Transformer backbone of PerAct from **Section 3.3**.\n",
        "\n",
        "The input grid is 100×100×100 = 1 million voxels. If we extract 5×5×5 patches, the input is still 20×20×20 = 8000 embeddings long. This sequence is way too long for a standard Transformer with O(n^2) self-attention connections. So we use the [PerceiverIO architecture](https://arxiv.org/abs/2107.14795) instead.  \n",
        "\n",
        "Perceiver uses a small set of **latent vectors** to encode the input. These latent vectors are randomly initialized and trained end-to-end. This approach decouples the depth of the Transformer self-attention layers from the dimensionality of the input space, which allows us train PerAct on very large input voxel grids. We can potentially scale the input to 200^3 voxels without increasing self-attention layer parameters.\n",
        "\n",
        "Refer to **Appendix B** in the paper for additional details.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://peract.github.io/media/figures/perceiver.png\" alt=\"drawing\"  width=\"600\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyZlp2hHO2Ys"
      },
      "source": [
        "But first, let's define some helper functions to implement the attention mechanism:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afrMAUZ3pp8q"
      },
      "outputs": [],
      "source": [
        "# From: https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_io.py\n",
        "\n",
        "from math import pi, log\n",
        "from functools import wraps\n",
        "\n",
        "from einops import rearrange, repeat, reduce\n",
        "from einops.layers.torch import Reduce\n",
        "\n",
        "from arm.network_utils import Conv3DInceptionBlock, DenseBlock, SpatialSoftmax3D, Conv3DInceptionBlockUpsampleBlock, Conv3DBlock, Conv3DUpsampleBlock\n",
        "\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "\n",
        "def cache_fn(f):\n",
        "    cache = None\n",
        "\n",
        "    @wraps(f)\n",
        "    def cached_fn(*args, _cache=True, **kwargs):\n",
        "        if not _cache:\n",
        "            return f(*args, **kwargs)\n",
        "        nonlocal cache\n",
        "        if cache is not None:\n",
        "            return cache\n",
        "        cache = f(*args, **kwargs)\n",
        "        return cache\n",
        "\n",
        "    return cached_fn\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(self.norm_context):\n",
        "            context = kwargs['context']\n",
        "            normed_context = self.norm_context(context)\n",
        "            kwargs.update(context=normed_context)\n",
        "\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim=-1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module): # is all you need. Living up to its name.\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        # attention\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        # dropout\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl9ZW-oCQA6l"
      },
      "source": [
        "We use these attention modules to construct PerceiverIO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rYuWNtRPEBy"
      },
      "outputs": [],
      "source": [
        "# PerceiverIO adapted for 6-DoF manipulation\n",
        "\n",
        "class PerceiverIO(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            depth,                    # number of self-attention layers\n",
        "            iterations,               # number cross-attention iterations (PerceiverIO uses just 1)\n",
        "            voxel_size,               # N voxels per side (size: N*N*N)\n",
        "            initial_dim,              # 10 dimensions - dimension of the input sequence to be encoded\n",
        "            low_dim_size,             # 4 dimensions - proprioception: {gripper_open, left_finger_joint, right_finger_joint, timestep}\n",
        "            layer=0,\n",
        "            num_rotation_classes=72,  # 5 degree increments (5*72=360) for each of the 3-axis\n",
        "            num_grip_classes=2,       # open or not open\n",
        "            num_collision_classes=2,  # collisions allowed or not allowed\n",
        "            input_axis=3,             # 3D tensors have 3 axes\n",
        "            num_latents=512,          # number of latent vectors\n",
        "            im_channels=64,           # intermediate channel size\n",
        "            latent_dim=512,           # dimensions of latent vectors\n",
        "            cross_heads=1,            # number of cross-attention heads\n",
        "            latent_heads=8,           # number of latent heads\n",
        "            cross_dim_head=64,\n",
        "            latent_dim_head=64,\n",
        "            activation='relu',\n",
        "            weight_tie_layers=False,\n",
        "            input_dropout=0.1,\n",
        "            attn_dropout=0.1,\n",
        "            decoder_dropout=0.0,\n",
        "            voxel_patch_size=5,       # intial patch size\n",
        "            voxel_patch_stride=5,     # initial stride to patchify voxel input\n",
        "            final_dim=64,             # final dimensions of features\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.layer = layer\n",
        "        self.init_dim = int(initial_dim)\n",
        "        self.iterations = iterations\n",
        "        self.input_axis = input_axis\n",
        "        self.voxel_size = voxel_size\n",
        "        self.low_dim_size = low_dim_size\n",
        "        self.im_channels = im_channels\n",
        "        self.voxel_patch_size = voxel_patch_size\n",
        "        self.voxel_patch_stride = voxel_patch_stride\n",
        "        self.num_rotation_classes = num_rotation_classes\n",
        "        self.num_grip_classes = num_grip_classes\n",
        "        self.num_collision_classes = num_collision_classes\n",
        "        self.final_dim = final_dim\n",
        "        self.input_dropout = input_dropout\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.decoder_dropout = decoder_dropout\n",
        "\n",
        "        # patchified input dimensions\n",
        "        spatial_size = voxel_size // self.voxel_patch_stride # 100/5 = 20\n",
        "\n",
        "        # 64 voxel features + 64 proprio features\n",
        "        self.input_dim_before_seq = self.im_channels * 2\n",
        "\n",
        "        # learnable positional encoding\n",
        "        lang_emb_dim, lang_max_seq_len = 512, 77\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1,\n",
        "                                                     lang_max_seq_len+spatial_size**3,\n",
        "                                                     self.input_dim_before_seq))\n",
        "\n",
        "        # voxel input preprocessing encoder\n",
        "        self.input_preprocess = Conv3DBlock(\n",
        "            self.init_dim, self.im_channels, kernel_sizes=1, strides=1,\n",
        "            norm=None, activation=activation,\n",
        "        )\n",
        "\n",
        "        # proprio preprocessing encoder\n",
        "        self.proprio_preprocess = DenseBlock(\n",
        "            self.low_dim_size, self.im_channels, norm=None, activation=activation,\n",
        "        )\n",
        "\n",
        "        # patchify conv\n",
        "        self.patchify = Conv3DBlock(\n",
        "            self.input_preprocess.out_channels, self.im_channels,\n",
        "            kernel_sizes=self.voxel_patch_size, strides=self.voxel_patch_stride,\n",
        "            norm=None, activation=activation)\n",
        "\n",
        "        # lang preprocess\n",
        "        self.lang_preprocess = nn.Linear(lang_emb_dim, self.im_channels * 2)\n",
        "\n",
        "        # pooling functions\n",
        "        self.local_maxp = nn.MaxPool3d(3, 2, padding=1)\n",
        "        self.global_maxp = nn.AdaptiveMaxPool3d(1)\n",
        "\n",
        "        # 1st 3D softmax\n",
        "        self.ss0 = SpatialSoftmax3D(\n",
        "            self.voxel_size, self.voxel_size, self.voxel_size,\n",
        "            self.im_channels)\n",
        "        flat_size = self.im_channels * 4\n",
        "\n",
        "        # latent vectors (that are randomly initialized)\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
        "\n",
        "        # encoder cross attention\n",
        "        self.cross_attend_blocks = nn.ModuleList([\n",
        "            PreNorm(latent_dim, Attention(latent_dim, self.input_dim_before_seq, heads=cross_heads,\n",
        "                                          dim_head=cross_dim_head, dropout=input_dropout),\n",
        "                    context_dim=self.input_dim_before_seq),\n",
        "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        ])\n",
        "\n",
        "        get_latent_attn = lambda: PreNorm(latent_dim,\n",
        "                                          Attention(latent_dim, heads=latent_heads,\n",
        "                                                    dim_head=latent_dim_head, dropout=attn_dropout))\n",
        "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
        "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
        "\n",
        "        # self-attention layers\n",
        "        self.layers = nn.ModuleList([])\n",
        "        cache_args = {'_cache': weight_tie_layers}\n",
        "\n",
        "        for i in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                get_latent_attn(**cache_args),\n",
        "                get_latent_ff(**cache_args)\n",
        "            ]))\n",
        "\n",
        "        # decoder cross attention\n",
        "        self.decoder_cross_attn = PreNorm(self.input_dim_before_seq, Attention(self.input_dim_before_seq, latent_dim, heads=cross_heads,\n",
        "                                                                      dim_head=cross_dim_head,\n",
        "                                                                      dropout=decoder_dropout),\n",
        "                                          context_dim=latent_dim)\n",
        "\n",
        "        # upsample conv\n",
        "        self.up0 = Conv3DUpsampleBlock(\n",
        "            self.input_dim_before_seq, self.final_dim,\n",
        "            kernel_sizes=self.voxel_patch_size, strides=self.voxel_patch_stride,\n",
        "            norm=None, activation=activation,\n",
        "        )\n",
        "\n",
        "        # 2nd 3D softmax\n",
        "        self.ss1 = SpatialSoftmax3D(\n",
        "            spatial_size, spatial_size, spatial_size,\n",
        "            self.input_dim_before_seq)\n",
        "\n",
        "        flat_size += self.input_dim_before_seq * 4\n",
        "\n",
        "        # final layers\n",
        "        self.final = Conv3DBlock(\n",
        "            self.im_channels * 2,\n",
        "            self.im_channels,\n",
        "            kernel_sizes=3,\n",
        "            strides=1, norm=None, activation=activation)\n",
        "\n",
        "        # 100x100x100x64 -> 100x100x100x1 decoder for translation Q-values\n",
        "        self.trans_decoder = Conv3DBlock(\n",
        "            self.final_dim, 1, kernel_sizes=3, strides=1,\n",
        "            norm=None, activation=None,\n",
        "        )\n",
        "\n",
        "        # final 3D softmax\n",
        "        self.ss_final = SpatialSoftmax3D(\n",
        "            self.voxel_size, self.voxel_size, self.voxel_size,\n",
        "            self.im_channels)\n",
        "\n",
        "        flat_size += self.im_channels * 4\n",
        "\n",
        "        # MLP layers\n",
        "        self.dense0 =  DenseBlock(\n",
        "            flat_size, 256, None, activation)\n",
        "        self.dense1 = DenseBlock(\n",
        "            256, self.final_dim, None, activation)\n",
        "\n",
        "        # 1x64 -> 1x(72+72+72+2+2) decoders for rotation, gripper open, and collision Q-values\n",
        "        self.rot_grip_collision_ff = DenseBlock(self.final_dim,\n",
        "                                          self.num_rotation_classes * 3 + \\\n",
        "                                          self.num_grip_classes + \\\n",
        "                                          self.num_collision_classes,\n",
        "                                          None, None)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            ins,\n",
        "            proprio,\n",
        "            lang_goal_embs,\n",
        "            bounds,\n",
        "            mask=None,\n",
        "    ):\n",
        "        # preprocess\n",
        "        d0 = self.input_preprocess(ins)               # [B,10,100,100,100] -> [B,64,100,100,100]\n",
        "\n",
        "        # aggregated features from 1st softmax and maxpool for MLP decoders\n",
        "        feats = [self.ss0(d0.contiguous()), self.global_maxp(d0).view(ins.shape[0], -1)]\n",
        "\n",
        "        # patchify input (5x5x5 patches)\n",
        "        ins = self.patchify(d0)                       # [B,64,100,100,100] -> [B,64,20,20,20]\n",
        "\n",
        "        b, c, d, h, w, device = *ins.shape, ins.device\n",
        "        axis = [d, h, w]\n",
        "        assert len(axis) == self.input_axis, 'input must have the same number of axis as input_axis'\n",
        "\n",
        "        # concat proprio\n",
        "        p = self.proprio_preprocess(proprio)          # [B,4] -> [B,64]\n",
        "        p = p.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, d, h, w)\n",
        "        ins = torch.cat([ins, p], dim=1)              # [B,128,20,20,20]\n",
        "\n",
        "        # channel last\n",
        "        ins = rearrange(ins, 'b d ... -> b ... d')    # [B,20,20,20,128]\n",
        "\n",
        "        # save original shape of input for layer\n",
        "        ins_orig_shape = ins.shape\n",
        "\n",
        "        # flatten voxel grid into sequence\n",
        "        ins = rearrange(ins, 'b ... d -> b (...) d')  # [B,8000,128]\n",
        "\n",
        "        # append language features as sequence\n",
        "        l = self.lang_preprocess(lang_goal_embs)      # [B,77,1024] -> [B,77,128]\n",
        "        ins = torch.cat((l, ins), dim=1)              # [B,8077,128]\n",
        "\n",
        "        # add learable pos encoding\n",
        "        ins = ins + self.pos_encoding\n",
        "\n",
        "        # batchify latents\n",
        "        x = repeat(self.latents, 'n d -> b n d', b=b)\n",
        "\n",
        "        cross_attn, cross_ff = self.cross_attend_blocks\n",
        "\n",
        "        for it in range(self.iterations):\n",
        "            # encoder cross attention\n",
        "            x = cross_attn(x, context=ins, mask=mask) + x\n",
        "            x = cross_ff(x) + x\n",
        "\n",
        "            # self-attention layers\n",
        "            for self_attn, self_ff in self.layers:\n",
        "                x = self_attn(x) + x\n",
        "                x = self_ff(x) + x\n",
        "\n",
        "        # decoder cross attention\n",
        "        latents = self.decoder_cross_attn(ins, context=x)\n",
        "        latents = latents[:, l.shape[1]:]\n",
        "\n",
        "        # reshape back to voxel grid\n",
        "        latents = latents.view(b, *ins_orig_shape[1:-1], latents.shape[-1])  # [B,20,20,20,64]\n",
        "        latents = rearrange(latents, 'b ... d -> b d ...')                   # [B,64,20,20,20]\n",
        "\n",
        "        # aggregated features from 2nd softmax and maxpool for MLP decoders\n",
        "        feats.extend([self.ss1(latents.contiguous()), self.global_maxp(latents).view(b, -1)])\n",
        "\n",
        "        # upsample layer\n",
        "        u0 = self.up0(latents)                         # [B,64,100,100,100]\n",
        "\n",
        "        # skip connection like in UNets\n",
        "        u = self.final(torch.cat([d0, u0], dim=1))     # [B,64+64,100,100,100] -> [B,64,100,100,100]\n",
        "\n",
        "        # translation decoder\n",
        "        trans = self.trans_decoder(u)                  # [B,64,100,100,100] -> [B,1,100,100,100]\n",
        "\n",
        "        # aggregated features from final softmax and maxpool for MLP decoders\n",
        "        feats.extend([self.ss_final(u.contiguous()), self.global_maxp(u).view(b, -1)])\n",
        "\n",
        "        # decoder MLP layers for rotation, gripper open, and collision\n",
        "        dense0 = self.dense0(torch.cat(feats, dim=1))\n",
        "        dense1 = self.dense1(dense0)                   # [B,72*3+2+2]\n",
        "\n",
        "        # format output\n",
        "        rot_and_grip_collision_out = self.rot_grip_collision_ff(dense1)\n",
        "        rot_and_grip_out = rot_and_grip_collision_out[:, :-self.num_collision_classes]\n",
        "        collision_out = rot_and_grip_collision_out[:, -self.num_collision_classes:]\n",
        "\n",
        "        return trans, rot_and_grip_out, collision_out\n",
        "\n",
        "\n",
        "# initialize PerceiverIO Transformer\n",
        "perceiver_encoder = PerceiverIO(\n",
        "    depth=6,\n",
        "    iterations=1,\n",
        "    voxel_size=VOXEL_SIZES[0],\n",
        "    initial_dim=3 + 3 + 1 + 3,\n",
        "    low_dim_size=4,\n",
        "    layer=0,\n",
        "    num_rotation_classes=72,\n",
        "    num_grip_classes=2,\n",
        "    num_collision_classes=2,\n",
        "    num_latents=NUM_LATENTS,\n",
        "    latent_dim=512,\n",
        "    cross_heads=1,\n",
        "    latent_heads=8,\n",
        "    cross_dim_head=64,\n",
        "    latent_dim_head=64,\n",
        "    weight_tie_layers=False,\n",
        "    activation='lrelu',\n",
        "    input_dropout=0.1,\n",
        "    attn_dropout=0.1,\n",
        "    decoder_dropout=0.0,\n",
        "    voxel_patch_size=5,\n",
        "    voxel_patch_stride=5,\n",
        "    final_dim=64,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbN4-D4ySSaK"
      },
      "source": [
        "### Q Functions\n",
        "\n",
        "Finally we put everything together to make PerAct's Q-Functions.  \n",
        "\n",
        "This module voxelizes RGB-D input, encodes per-voxel features, and predicts discretized actions.  \n",
        "\n",
        "<div>\n",
        "<img src=\"https://peract.github.io/media/figures/arch.png\" alt=\"drawing\"  width=\"1000\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQH8qnWG7caH"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class QFunction(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 perceiver_encoder: nn.Module,\n",
        "                 voxel_grid: VoxelGrid,\n",
        "                 rotation_resolution: float,\n",
        "                 device,\n",
        "                 training):\n",
        "        super(QFunction, self).__init__()\n",
        "        self._rotation_resolution = rotation_resolution\n",
        "        self._voxel_grid = voxel_grid\n",
        "        self._qnet = copy.deepcopy(perceiver_encoder)\n",
        "        self._qnet._dev = device\n",
        "\n",
        "    def _argmax_3d(self, tensor_orig):\n",
        "        b, c, d, h, w = tensor_orig.shape  # c will be one\n",
        "        idxs = tensor_orig.view(b, c, -1).argmax(-1)\n",
        "        indices = torch.cat([((idxs // h) // d), (idxs // h) % w, idxs % w], 1)\n",
        "        return indices\n",
        "\n",
        "    def choose_highest_action(self, q_trans, q_rot_grip, q_collision):\n",
        "        coords = self._argmax_3d(q_trans)\n",
        "        rot_and_grip_indicies = None\n",
        "        if q_rot_grip is not None:\n",
        "            q_rot = torch.stack(torch.split(\n",
        "                q_rot_grip[:, :-2],\n",
        "                int(360 // self._rotation_resolution),\n",
        "                dim=1), dim=1)\n",
        "            rot_and_grip_indicies = torch.cat(\n",
        "                [q_rot[:, 0:1].argmax(-1),\n",
        "                 q_rot[:, 1:2].argmax(-1),\n",
        "                 q_rot[:, 2:3].argmax(-1),\n",
        "                 q_rot_grip[:, -2:].argmax(-1, keepdim=True)], -1)\n",
        "            ignore_collision = q_collision[:, -2:].argmax(-1, keepdim=True)\n",
        "        return coords, rot_and_grip_indicies, ignore_collision\n",
        "\n",
        "    def forward(self,\n",
        "                obs,\n",
        "                proprio,\n",
        "                pcd,\n",
        "                lang_goal_embs,\n",
        "                bounds=None):\n",
        "\n",
        "        # flatten point cloud\n",
        "        bs = obs[0][0].shape[0]\n",
        "        pcd_flat = torch.cat(\n",
        "            [p.permute(0, 2, 3, 1).reshape(bs, -1, 3) for p in pcd], 1)\n",
        "\n",
        "        # flatten rgb\n",
        "        image_features = [o[0] for o in obs]\n",
        "        feat_size = image_features[0].shape[1]\n",
        "        flat_imag_features = torch.cat(\n",
        "            [p.permute(0, 2, 3, 1).reshape(bs, -1, feat_size) for p in\n",
        "             image_features], 1)\n",
        "\n",
        "        # voxelize\n",
        "        voxel_grid = self._voxel_grid.coords_to_bounding_voxel_grid(\n",
        "            pcd_flat, coord_features=flat_imag_features, coord_bounds=bounds)\n",
        "\n",
        "        # swap to channels fist\n",
        "        voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach()\n",
        "\n",
        "        # batch bounds if necessary\n",
        "        if bounds.shape[0] != bs:\n",
        "            bounds = bounds.repeat(bs, 1)\n",
        "\n",
        "        # forward pass\n",
        "        q_trans, rot_and_grip_q, collision_q = self._qnet(voxel_grid,\n",
        "                                                          proprio,\n",
        "                                                          lang_goal_embs,\n",
        "                                                          bounds)\n",
        "        return q_trans, rot_and_grip_q, collision_q, voxel_grid\n",
        "\n",
        "    def latents(self):\n",
        "        return self._qnet.latent_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70Fs_PQz0Cx"
      },
      "source": [
        "### PerAct Agent\n",
        "\n",
        "Let's initialize PerAct and define an update function for the training loop.\n",
        "\n",
        "The keyframe actions used for supervision are represented as one-hot vectors. Then we use cross-entropy loss to train PerAct, just like a standard classifier. This training method is also closely related to [Energy-Based Models (EBMs)](https://arxiv.org/abs/2109.00137)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8BqH-grZvoZ"
      },
      "outputs": [],
      "source": [
        "from arm.optim.lamb import Lamb\n",
        "from arm.utils import stack_on_channel\n",
        "\n",
        "\n",
        "class PerceiverActorAgent():\n",
        "    def __init__(self,\n",
        "                coordinate_bounds: list,\n",
        "                perceiver_encoder: nn.Module,\n",
        "                camera_names: list,\n",
        "                batch_size: int,\n",
        "                voxel_size: int,\n",
        "                voxel_feature_size: int,\n",
        "                num_rotation_classes: int,\n",
        "                rotation_resolution: float,\n",
        "                lr: float = 0.0001,\n",
        "                image_resolution: list = None,\n",
        "                lambda_weight_l2: float = 0.0,\n",
        "                transform_augmentation: bool = True,\n",
        "                transform_augmentation_xyz: list = [0.0, 0.0, 0.0],\n",
        "                transform_augmentation_rpy: list = [0.0, 0.0, 180.0],\n",
        "                transform_augmentation_rot_resolution: int = 5,\n",
        "                optimizer_type: str = 'lamb'):\n",
        "\n",
        "        self._coordinate_bounds = coordinate_bounds\n",
        "        self._perceiver_encoder = perceiver_encoder\n",
        "        self._camera_names = camera_names\n",
        "        self._batch_size = batch_size\n",
        "        self._voxel_size = voxel_size\n",
        "        self._voxel_feature_size = voxel_feature_size\n",
        "        self._num_rotation_classes = num_rotation_classes\n",
        "        self._rotation_resolution = rotation_resolution\n",
        "        self._lr = lr\n",
        "        self._image_resolution = image_resolution\n",
        "        self._lambda_weight_l2 = lambda_weight_l2\n",
        "        self._transform_augmentation = transform_augmentation\n",
        "        self._transform_augmentation_xyz = transform_augmentation_xyz\n",
        "        self._transform_augmentation_rpy = transform_augmentation_rpy\n",
        "        self._transform_augmentation_rot_resolution = transform_augmentation_rot_resolution\n",
        "        self._optimizer_type = optimizer_type\n",
        "\n",
        "        self._cross_entropy_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def build(self, training: bool, device: torch.device = None):\n",
        "        self._training = training\n",
        "        self._device = device\n",
        "\n",
        "        vox_grid = VoxelGrid(\n",
        "            coord_bounds=self._coordinate_bounds,\n",
        "            voxel_size=self._voxel_size,\n",
        "            device=device,\n",
        "            batch_size=self._batch_size,\n",
        "            feature_size=self._voxel_feature_size,\n",
        "            max_num_coords=np.prod([IMAGE_SIZE, IMAGE_SIZE]) * len(CAMERAS),\n",
        "        )\n",
        "        self._vox_grid = vox_grid\n",
        "\n",
        "        self._q = QFunction(self._perceiver_encoder,\n",
        "                            vox_grid,\n",
        "                            self._rotation_resolution,\n",
        "                            device,\n",
        "                            training).to(device).train(training)\n",
        "\n",
        "        self._coordinate_bounds = torch.tensor(self._coordinate_bounds,\n",
        "                                               device=device).unsqueeze(0)\n",
        "\n",
        "        if self._optimizer_type == 'lamb':\n",
        "            # From: https://github.com/cybertronai/pytorch-lamb/blob/master/pytorch_lamb/lamb.py\n",
        "            self._optimizer = Lamb(\n",
        "                self._q.parameters(),\n",
        "                lr=self._lr,\n",
        "                weight_decay=self._lambda_weight_l2,\n",
        "                betas=(0.9, 0.999),\n",
        "                adam=False,\n",
        "            )\n",
        "        elif self._optimizer_type == 'adam':\n",
        "            self._optimizer = torch.optim.Adam(\n",
        "                self._q.parameters(),\n",
        "                lr=self._lr,\n",
        "                weight_decay=self._lambda_weight_l2,\n",
        "            )\n",
        "        else:\n",
        "            raise Exception('Unknown optimizer')\n",
        "\n",
        "    def _softmax_q(self, q):\n",
        "        q_shape = q.shape\n",
        "        return F.softmax(q.reshape(q_shape[0], -1), dim=1).reshape(q_shape)\n",
        "\n",
        "    def _get_one_hot_expert_actions(self,  # You don't really need this function since GT labels are already in the right format. This is some leftover code from my experiments with label smoothing.\n",
        "                                    batch_size,\n",
        "                                    action_trans,\n",
        "                                    action_rot_grip,\n",
        "                                    action_ignore_collisions,\n",
        "                                    device):\n",
        "        bs = batch_size\n",
        "\n",
        "        # initialize with zero tensors\n",
        "        action_trans_one_hot = torch.zeros((bs, self._voxel_size, self._voxel_size, self._voxel_size), dtype=int, device=device)\n",
        "        action_rot_x_one_hot = torch.zeros((bs, self._num_rotation_classes), dtype=int, device=device)\n",
        "        action_rot_y_one_hot = torch.zeros((bs, self._num_rotation_classes), dtype=int, device=device)\n",
        "        action_rot_z_one_hot = torch.zeros((bs, self._num_rotation_classes), dtype=int, device=device)\n",
        "        action_grip_one_hot  = torch.zeros((bs, 2), dtype=int, device=device)\n",
        "        action_collision_one_hot = torch.zeros((bs, 2), dtype=int, device=device)\n",
        "\n",
        "        # fill one-hots\n",
        "        for b in range(bs):\n",
        "          # translation\n",
        "          gt_coord = action_trans[b, :]\n",
        "          action_trans_one_hot[b, gt_coord[0], gt_coord[1], gt_coord[2]] = 1\n",
        "\n",
        "          # rotation\n",
        "          gt_rot_grip = action_rot_grip[b, :]\n",
        "          action_rot_x_one_hot[b, gt_rot_grip[0]] = 1\n",
        "          action_rot_y_one_hot[b, gt_rot_grip[1]] = 1\n",
        "          action_rot_z_one_hot[b, gt_rot_grip[2]] = 1\n",
        "          action_grip_one_hot[b, gt_rot_grip[3]] = 1\n",
        "\n",
        "          # ignore collision\n",
        "          gt_ignore_collisions = action_ignore_collisions[b, :]\n",
        "          action_collision_one_hot[b, gt_ignore_collisions[0]] = 1\n",
        "\n",
        "        # flatten trans\n",
        "        action_trans_one_hot = action_trans_one_hot.view(bs, -1)\n",
        "\n",
        "        return action_trans_one_hot, \\\n",
        "               action_rot_x_one_hot, \\\n",
        "               action_rot_y_one_hot, \\\n",
        "               action_rot_z_one_hot, \\\n",
        "               action_grip_one_hot,  \\\n",
        "               action_collision_one_hot\n",
        "\n",
        "\n",
        "    def update(self, step: int, replay_sample: dict, backprop: bool = True) -> dict:\n",
        "        # sample\n",
        "        action_trans = replay_sample['trans_action_indicies'][:, -1, :3].int()\n",
        "        action_rot_grip = replay_sample['rot_grip_action_indicies'][:, -1].int()\n",
        "        action_ignore_collisions = replay_sample['ignore_collisions'][:, -1].int()\n",
        "        action_gripper_pose = replay_sample['gripper_pose'][:, -1]\n",
        "        lang_goal_embs = replay_sample['lang_goal_embs'][:, -1].float()\n",
        "\n",
        "        # metric scene bounds\n",
        "        bounds = bounds_tp1 = self._coordinate_bounds\n",
        "\n",
        "        # inputs\n",
        "        proprio = stack_on_channel(replay_sample['low_dim_state'])\n",
        "        obs, pcd = _preprocess_inputs(replay_sample)\n",
        "\n",
        "        # TODO: data augmentation by applying SE(3) pertubations to pcd and actions\n",
        "        # see https://github.com/peract/peract/blob/main/voxel/augmentation.py#L68 for reference\n",
        "\n",
        "        # Q function\n",
        "        q_trans, rot_grip_q, collision_q, voxel_grid = self._q(obs,\n",
        "                                                               proprio,\n",
        "                                                               pcd,\n",
        "                                                               lang_goal_embs,\n",
        "                                                               bounds)\n",
        "\n",
        "        # one-hot expert actions\n",
        "        bs = self._batch_size\n",
        "        action_trans_one_hot, action_rot_x_one_hot, \\\n",
        "        action_rot_y_one_hot, action_rot_z_one_hot, \\\n",
        "        action_grip_one_hot, action_collision_one_hot = self._get_one_hot_expert_actions(bs,\n",
        "                                                                                         action_trans,\n",
        "                                                                                         action_rot_grip,\n",
        "                                                                                         action_ignore_collisions,\n",
        "                                                                                         device=self._device)\n",
        "        total_loss = 0.\n",
        "        if backprop:\n",
        "            # cross-entropy loss\n",
        "            trans_loss = self._cross_entropy_loss(q_trans.view(bs, -1),\n",
        "                                                  action_trans_one_hot.argmax(-1))\n",
        "\n",
        "            rot_grip_loss = 0.\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 0*self._num_rotation_classes:1*self._num_rotation_classes],\n",
        "                                                      action_rot_x_one_hot.argmax(-1))\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 1*self._num_rotation_classes:2*self._num_rotation_classes],\n",
        "                                                      action_rot_y_one_hot.argmax(-1))\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 2*self._num_rotation_classes:3*self._num_rotation_classes],\n",
        "                                                      action_rot_z_one_hot.argmax(-1))\n",
        "            rot_grip_loss += self._cross_entropy_loss(rot_grip_q[:, 3*self._num_rotation_classes:],\n",
        "                                                      action_grip_one_hot.argmax(-1))\n",
        "\n",
        "            collision_loss = self._cross_entropy_loss(collision_q,\n",
        "                                                      action_collision_one_hot.argmax(-1))\n",
        "\n",
        "            total_loss = trans_loss + rot_grip_loss + collision_loss\n",
        "            total_loss = total_loss.mean()\n",
        "\n",
        "            # backprop\n",
        "            self._optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            self._optimizer.step()\n",
        "\n",
        "            total_loss = total_loss.item()\n",
        "\n",
        "        # choose best action through argmax\n",
        "        coords_indicies, rot_and_grip_indicies, ignore_collision_indicies = self._q.choose_highest_action(q_trans,\n",
        "                                                                                                          rot_grip_q,\n",
        "                                                                                                          collision_q)\n",
        "\n",
        "        # discrete to continuous translation action\n",
        "        res = (bounds[:, 3:] - bounds[:, :3]) / self._voxel_size\n",
        "        continuous_trans = bounds[:, :3] + res * coords_indicies.int() + res / 2\n",
        "\n",
        "        return {\n",
        "            'total_loss': total_loss,\n",
        "            'voxel_grid': voxel_grid,\n",
        "            'q_trans': self._softmax_q(q_trans),\n",
        "            'pred_action': {\n",
        "                'trans': coords_indicies,\n",
        "                'continuous_trans': continuous_trans,\n",
        "                'rot_and_grip': rot_and_grip_indicies,\n",
        "                'collision': ignore_collision_indicies\n",
        "            },\n",
        "            'expert_action': {\n",
        "                'action_trans': action_trans\n",
        "            }\n",
        "        }\n",
        "\n",
        "# initialize PerceiverActor\n",
        "peract_agent = PerceiverActorAgent(\n",
        "    coordinate_bounds=SCENE_BOUNDS,\n",
        "    perceiver_encoder=perceiver_encoder,\n",
        "    camera_names=CAMERAS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    voxel_size=VOXEL_SIZES[0],\n",
        "    voxel_feature_size=3,\n",
        "    num_rotation_classes=72,\n",
        "    rotation_resolution=5,\n",
        "    lr=0.0001,\n",
        "    image_resolution=[IMAGE_SIZE, IMAGE_SIZE],\n",
        "    lambda_weight_l2=0.000001,\n",
        "    transform_augmentation=False,\n",
        "    optimizer_type='lamb',\n",
        ")\n",
        "peract_agent.build(training=True, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG1IZxjsz7Om"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "The final training loop samples data from the replay buffer and trains the agent with supervised learning. 2400 iterations should take ~130mins.\n",
        "\n",
        "❗2400 iterations is probably not enough for robust performance, so you might see some weird predictions. Training for longer periods, particularly with data augmentation (see **Appendix E**), will improve performance. But we will stick with this for now to avoid Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC-6HBZOv0PW",
        "outputId": "722a78df-81a8-40ca-ac9c-7bbc37fdc8ad"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "LOG_FREQ = 50\n",
        "TRAINING_ITERATIONS = 2400\n",
        "\n",
        "start_time = time.time()\n",
        "for iteration in range(TRAINING_ITERATIONS):\n",
        "    batch = next(train_data_iter)\n",
        "    batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
        "    update_dict = peract_agent.update(iteration, batch)\n",
        "\n",
        "    if iteration % LOG_FREQ == 0:\n",
        "      elapsed_time = (time.time() - start_time) / 60.0\n",
        "      print(\"Total Loss: %f | Elapsed Time: %f mins\" % (update_dict['total_loss'], elapsed_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpJPZzfcUV96"
      },
      "source": [
        "## Inference and Visualization\n",
        "\n",
        "Let's see how PerAct does on held-out test data.  \n",
        "\n",
        "PerAct should be evaluated in simulation on scenes with randomized object poses and object instances. But this Colab notebook doesn't support the V-REP simulator (for RLBench tasks). So for now we will do inference on a static test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehbeZdHdEwVC"
      },
      "outputs": [],
      "source": [
        "from arm.utils import visualise_voxel, discrete_euler_to_quaternion, get_gripper_render_pose\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "batch = next(test_data_iter)\n",
        "lang_goal = batch['lang_goal'][0][0][0]\n",
        "batch = {k: v.to(device) for k, v in batch.items() if type(v) == torch.Tensor}\n",
        "update_dict = peract_agent.update(iteration, batch, backprop=False)\n",
        "\n",
        "# things to visualize\n",
        "vis_voxel_grid = update_dict['voxel_grid'][0].detach().cpu().numpy()\n",
        "vis_trans_q = update_dict['q_trans'][0].detach().cpu().numpy()\n",
        "vis_trans_coord = update_dict['pred_action']['trans'][0].detach().cpu().numpy()\n",
        "vis_gt_coord = update_dict['expert_action']['action_trans'][0].detach().cpu().numpy()\n",
        "\n",
        "# discrete to continuous\n",
        "continuous_trans = update_dict['pred_action']['continuous_trans'][0].detach().cpu().numpy()\n",
        "continuous_quat = discrete_euler_to_quaternion(update_dict['pred_action']['rot_and_grip'][0][:3].detach().cpu().numpy(),\n",
        "                                               resolution=peract_agent._rotation_resolution)\n",
        "gripper_open = bool(update_dict['pred_action']['rot_and_grip'][0][-1].detach().cpu().numpy())\n",
        "ignore_collision = bool(update_dict['pred_action']['collision'][0][0].detach().cpu().numpy())\n",
        "\n",
        "# gripper visualization pose\n",
        "voxel_size = 0.045\n",
        "voxel_scale = voxel_size * 100\n",
        "gripper_pose_mat = get_gripper_render_pose(voxel_scale,\n",
        "                                           SCENE_BOUNDS[:3],\n",
        "                                           continuous_trans,\n",
        "                                           continuous_quat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "iL-RyskRiA_u",
        "outputId": "54258292-1d7d-427f-9940-9f41fca5554f"
      },
      "outputs": [],
      "source": [
        "#@markdown #### Show Q-Prediction and Best Action\n",
        "show_expert_action = False  #@param {type:\"boolean\"}\n",
        "show_q_values = True  #@param {type:\"boolean\"}\n",
        "render_gripper = True  #@param {type:\"boolean\"}\n",
        "rotation_amount = 0 #@param {type:\"slider\", min:-180, max:180, step:5}\n",
        "\n",
        "rendered_img = visualise_voxel(vis_voxel_grid,\n",
        "                               vis_trans_q if show_q_values else None,\n",
        "                               vis_trans_coord,\n",
        "                               vis_gt_coord if show_expert_action else None,\n",
        "                               voxel_size=voxel_size,\n",
        "                               rotation_amount=np.deg2rad(rotation_amount),\n",
        "                               render_gripper=render_gripper,\n",
        "                               gripper_pose=gripper_pose_mat,\n",
        "                               gripper_mesh_scale=voxel_scale)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "plt.imshow(rendered_img)\n",
        "plt.axis('off')\n",
        "\n",
        "print(f\"Lang Goal: {lang_goal}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUM917haS8MZ"
      },
      "source": [
        "The red values are normalized translation Q-predictions (they might be barely visible if the distribution is peaky). The blue voxel is the expert keyframe action. And the turquoise gripper (without fingers) is the 6-DoF action with the highest Q-values.\n",
        "\n",
        "That's it! 🎉 Now use your favorite motion-planner to reach the 6-DoF pose. Once the pose is reached, you can repeat the process again with the new observation until the desired goal has been reached.\n",
        "\n",
        "The full implementation is not that different to what has been covered in this notebook. Major differences include: the replay buffer is loaded with lots of tasks, the agent is trained with larger batch-sizes by using multiple GPUs, and the agent is trained for a much longer period (with data augmentation) to optimize on all the tasks.\n",
        "\n",
        "This framework might be applicable to problems beyond just single-arm manipulation. For instance, you can predict two gripper poses for bimanual manipulation, or waypoint poses for navigation, or foothold poses for legged locomotion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALTLI39HgPbd"
      },
      "source": [
        "## Citations\n",
        "\n",
        "**PerAct**\n",
        "```\n",
        "@inproceedings{shridhar2022peract,\n",
        "  title     = {Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation},\n",
        "  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},\n",
        "  booktitle = {Proceedings of the 6th Conference on Robot Learning (CoRL)},\n",
        "  year      = {2022},\n",
        "}\n",
        "```\n",
        "\n",
        "**C2FARM**\n",
        "```\n",
        "@inproceedings{james2022coarse,\n",
        "  title={Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation},\n",
        "  author={James, Stephen and Wada, Kentaro and Laidlow, Tristan and Davison, Andrew J},\n",
        "  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n",
        "  pages={13739--13748},\n",
        "  year={2022}\n",
        "}\n",
        "```\n",
        "\n",
        "**PerceiverIO**\n",
        "```\n",
        "@article{jaegle2021perceiver,\n",
        "  title={Perceiver io: A general architecture for structured inputs \\& outputs},\n",
        "  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},\n",
        "  journal={arXiv preprint arXiv:2107.14795},\n",
        "  year={2021}\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "**RLBench**\n",
        "```\n",
        "@article{james2020rlbench,\n",
        "  title={Rlbench: The robot learning benchmark \\& learning environment},\n",
        "  author={James, Stephen and Ma, Zicong and Arrojo, David Rovick and Davison, Andrew J},\n",
        "  journal={IEEE Robotics and Automation Letters},\n",
        "  volume={5},\n",
        "  number={2},\n",
        "  pages={3019--3026},\n",
        "  year={2020},\n",
        "  publisher={IEEE}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
